{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.datasets import mnist\n",
    "from tensorflow.python.keras.models import Sequential,load_model,Model\n",
    "from tensorflow.python.keras.layers import Dense,Activation,Dropout,Conv2D,AvgPool2D,Flatten,MaxPool2D,BatchNormalization,GlobalAvgPool2D\n",
    "from tensorflow.python.keras.optimizers import RMSprop,Adam,SGD\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from tensorflow.python.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.python.keras.applications import MobileNetV2,InceptionResNetV2\n",
    "from tensorflow.python.keras.applications.resnet import ResNet101,ResNet152,ResNet50\n",
    "from tensorflow.python.keras.applications import VGG16,VGG19\n",
    "from tensorflow.python.keras.preprocessing.image import load_img\n",
    "from tensorflow.python.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x,train_y),(test_x,test_y)=mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of train_x is: (60000, 28, 28)\n",
      "The size of train_y is: (60000,)\n",
      "The size of test_x is: (10000, 28, 28)\n",
      "The size of test_y is: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"The size of train_x is: {}\".format(train_x.shape))\n",
    "print(\"The size of train_y is: {}\".format(train_y.shape))\n",
    "\n",
    "print(\"The size of test_x is: {}\".format(test_x.shape))\n",
    "print(\"The size of test_y is: {}\".format(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshaping the Image values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "train_x = train_x.reshape(train_x.shape[0], 28, 28, 1)\n",
    "test_x = test_x.reshape(test_x.shape[0], 28, 28, 1)\n",
    "print(train_x.shape)\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing the values of the Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x/255.0\n",
    "test_x = test_x/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding The lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "train_y = to_categorical(train_y, num_classes=10)\n",
    "test_y = to_categorical(test_y, num_classes=10)\n",
    "print(train_y.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 24, 24, 6)         156       \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 12, 12, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 8, 8, 16)          2416      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               30840     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 44,426\n",
      "Trainable params: 44,426\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Adding a Convolution Layer C1\n",
    "# Input shape = N = (28 x 28)\n",
    "# No. of filters  = 6\n",
    "# Filter size = f = (5 x 5)\n",
    "# Padding = P = 0\n",
    "# Strides = S = 1\n",
    "# Size of each feature map in C1 is (N-f+2P)/S +1 = 28-5+1 = 24\n",
    "# No. of parameters between input layer and C1 = (5*5 + 1)*6 = 156\n",
    "model.add(Conv2D(filters=6, kernel_size=(5,5), padding='valid', input_shape=(28,28,1), activation='tanh'))\n",
    "\n",
    "# Adding an Average Pooling Layer S2\n",
    "# Input shape = N = (24 x 24)\n",
    "# No. of filters = 6\n",
    "# Filter size = f = (2 x 2)\n",
    "# Padding = P = 0\n",
    "# Strides = S = 2\n",
    "# Size of each feature map in S2 is (N-f+2P)/S +1 = (24-2+0)/2+1 = 11+1 = 12\n",
    "# No. of parameters between C1 and S2 = (1+1)*6 = 12\n",
    "model.add(AvgPool2D(pool_size=(2,2)))\n",
    "\n",
    "# Adding a Convolution Layer C3\n",
    "# Input shape = N = (12 x 12)\n",
    "# No. of filters  = 16\n",
    "# Filter size = f = (5 x 5)\n",
    "# Padding = P = 0\n",
    "# Strides = S = 1\n",
    "# Size of each feature map in C3 is (N-f+2P)/S +1 = 12-5+1 = 8\n",
    "# No. of parameters between S2 and C3 = (5*5*6*16 + 16) + 16 = 2416\n",
    "model.add(Conv2D(filters=16, kernel_size=(5,5), padding='valid', activation='tanh'))\n",
    "\n",
    "# Adding an Average Pooling Layer S4\n",
    "# Input shape = N = (8 x 8)\n",
    "# No. of filters = 16\n",
    "# Filter size = f = (2 x 2)\n",
    "# Padding = P = 0\n",
    "# Strides = S = 2\n",
    "# Size of each feature map in S4 is (N-f+2P)/S +1 = (8-2+0)/2+1 = 3+1 = 4\n",
    "# No. of parameters between C3 and S4 = (1+1)*16 = 32\n",
    "model.add(AvgPool2D(pool_size=(2,2)))\n",
    "\n",
    "# As compared to LeNet-5 architecture there was one more application of convolution but in our code  further application of \n",
    "# convolution with (5 x 5) filter would result in a negative dimension which is not possible. So we aren't applying any more\n",
    "# convolution here.\n",
    "\n",
    "# Flattening the layer S4\n",
    "# There would be 16*(4*4) = 256 neurons\n",
    "model.add(Flatten())\n",
    "\n",
    "# Adding a Dense layer with `tanh` activation+# \n",
    "# No. of inputs = 256\n",
    "# No. of outputs = 120\n",
    "# No. of parameters = 256*120 + 120 = 30,840\n",
    "model.add(Dense(120, activation='tanh'))\n",
    "\n",
    "# Adding a Dense layer with `tanh` activation\n",
    "# No. of inputs = 120\n",
    "# No. of outputs = 84\n",
    "# No. of parameters = 120*84 + 84 = 10,164\n",
    "model.add(Dense(84, activation='tanh'))\n",
    "\n",
    "# Adding a Dense layer with `softmax` activation\n",
    "# No. of inputs = 84\n",
    "# No. of outputs = 10\n",
    "# No. of parameters = 84*10 + 10 = 850\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 33s 545us/sample - loss: 0.3518 - acc: 0.8996 - val_loss: 0.1617 - val_acc: 0.9506\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 33s 554us/sample - loss: 0.1346 - acc: 0.9581 - val_loss: 0.1035 - val_acc: 0.9682\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 33s 546us/sample - loss: 0.0899 - acc: 0.9726 - val_loss: 0.0742 - val_acc: 0.9769\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 33s 551us/sample - loss: 0.0668 - acc: 0.9796 - val_loss: 0.0629 - val_acc: 0.9808\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 32s 528us/sample - loss: 0.0525 - acc: 0.9839 - val_loss: 0.0609 - val_acc: 0.9801\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 32s 531us/sample - loss: 0.0439 - acc: 0.9866 - val_loss: 0.0539 - val_acc: 0.9823\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 32s 527us/sample - loss: 0.0380 - acc: 0.9883 - val_loss: 0.0468 - val_acc: 0.9854\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 31s 522us/sample - loss: 0.0309 - acc: 0.9904 - val_loss: 0.0484 - val_acc: 0.9841\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 33s 548us/sample - loss: 0.0269 - acc: 0.9916 - val_loss: 0.0423 - val_acc: 0.9879\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 32s 532us/sample - loss: 0.0232 - acc: 0.9932 - val_loss: 0.0428 - val_acc: 0.9866\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 33s 542us/sample - loss: 0.0199 - acc: 0.9941 - val_loss: 0.0463 - val_acc: 0.9856\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 37s 620us/sample - loss: 0.0173 - acc: 0.9947 - val_loss: 0.0432 - val_acc: 0.9866\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 68s 1ms/sample - loss: 0.0155 - acc: 0.9954 - val_loss: 0.0383 - val_acc: 0.9871\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 14s 240us/sample - loss: 0.0144 - acc: 0.9954 - val_loss: 0.0425 - val_acc: 0.9868\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 13s 209us/sample - loss: 0.0113 - acc: 0.9967 - val_loss: 0.0466 - val_acc: 0.9851\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 13s 209us/sample - loss: 0.0104 - acc: 0.9969 - val_loss: 0.0412 - val_acc: 0.9877\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 13s 213us/sample - loss: 0.0098 - acc: 0.9971 - val_loss: 0.0474 - val_acc: 0.9864\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 15s 242us/sample - loss: 0.0083 - acc: 0.9976 - val_loss: 0.0418 - val_acc: 0.98798\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 14s 232us/sample - loss: 0.0075 - acc: 0.9977 - val_loss: 0.0465 - val_acc: 0.9868\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 14s 240us/sample - loss: 0.0089 - acc: 0.9971 - val_loss: 0.0452 - val_acc: 0.9876\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16a77055bc8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_x, train_y, batch_size=128, epochs=20, verbose=1, validation_data=(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 105us/sample - loss: 0.0452 - acc: 0.9876\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.0066 - acc: 0.9982\n",
      "Test Loss: 0.04516613108418678\n",
      "Test accuracy: 0.9876\n",
      "Train Loss 0.006585052784008925\n",
      "Train accuracy 0.9981667\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_x, test_y)\n",
    "trainscore=model.evaluate(train_x,train_y)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(\"Train Loss\",trainscore[0])\n",
    "print(\"Train accuracy\",trainscore[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model.save(\"my_model.h5\")\n",
    "later_model=load_model(\"my_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
